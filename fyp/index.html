<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Improve learning resource recommendations for students</title>

		<meta name="description" content="An evaluation of machine learning algorihtms on a educational corpus">
		<meta name="author" content="Cathal Geoghegan">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1><small>Improve learning resource recommendations for students</small></h1>
					<p>
						<small>Created by <a href="http://CathalGeoghegan.me">Cathal Geoghegan</a> / <a href="http://twitter.com/_Kotl">@_Kotl</a></small>
					</p>
				</section>
				<section>
					<section>
						<h3>Introduction</h3>
						<p class="fragment">
							I have been investigating the use of machine learning algorithms to generate recommendations from an educational corpus. Focusing mainly on LDA, k-NN and Word2Vec.
						</p>
						<p class="fragment">
							Specifically I have been investigating the temporal performance and the quality of recommendations generated by the above models on a collection of 30,000 papers from arXiv.
						</p>
					</section>
					<section>
						<h3>Motivation</h3>
						<p class="fragment">
							The internet has proved an invaluable tool in providing access to knowledge. However the volume of data is far too large to be finely catalogued by humans. For this we need to use machines to automate the task.
						</p>
						<p class="fragment">
							The overall aim of this is to try and choose a model to use when generating recommendations for students.
						</p>
					</section>
				</section>
				<section>
					<h3>Outline</h3>
					<ul>
						<li class="fragment">What is machine learning</li>
						<li class="fragment">What is currently being done in this field</li>
						<li class="fragment">What I hope to achieve with my FYP</li>
						<li class="fragment">Preliminary results</li>
						<li class="fragment">Questions</li>
					</ul>
				</section>
				<section>
					<section>
						<h3>Machine Learning</h3>
						<p class="fragment">
							Machine learning is an interdisciplinary field dedicated to the study of self learning systems. There are three types of machine learning.
						</p>
						<ul>
							<li class="fragment">Supervised Learning</li>
							<li class="fragment">Reinforced Learning</li>
							<li class="fragment">Unsupervised Learning</li>
						</ul>
					</section>
					<section>
						<h3>Supervised Learning</h3>
						<p class="fragment">
							The machine is fed inputs <em>x1, x2...xZ</em>,
							desired outputs <em>y1, y2...yZ</em> and then tries to create a model which fits inputs to outputs.
						</p>
						<p class="fragment">
							A large number of training examples are required to properly train a supervised model. For this reason I choose not to use this approach.
						</p>
					</section>
					<section>
						<h3>Reinforced Learning</h3>
						<p class="fragment">
							The machine interacts with the environment and produces actions <em>a1, a2...aZ</em> which affects the environment. The machine then receives rewards <em>r1, r2...rZ</em> and then tries to learn to produce actions which maximises future rewards.
						</p>
						<p class="fragment">
							Like supervised learning this would require a large number of training samples to produce a working model and was not used in this project.
						</p>
					</section>
					<section>
						<h3>Unsupervised Learning</h3>
						<p class="fragment">
							The machine is just fed inputs <em>x1, x2...xZ</em> and tries to build a model that finds patterns in the data. No desired outputs nor rewards are needed.
						</p>
						<p class="fragment">
							This type of machine learning is the focus of my project. Each of the algorithms used builds a probabilistic model of the data which it can then use for inference.
						</p>
					</section>
				</section>
				<section>
					<section>
						<h3>State of the Art</h3>
						<ul>
							<li class="fragment">1990, Deerwester introduced the concept of latent semantic indexing.</li>
							<li class="fragment">2003, Blei and Ng built upon the work done previously by Deerwester to discover Latent dirichlet allocation.</li>
							<li class="fragment">2008, Knowledge and Information Systems listed k-NN as one of the top data-mining algorithms.</li>
							<li class="fragment">2014, Word2Vec released as an efficient algorithm to calculate vector representations of words.
						</ul>
					</section>
					<section>
						<h3>Technology</h3>
						<p class="fragment">
							<em>Gensim</em>: a python library to conduct unsupervised semantic modeling of plain text.
						</p>
						<p class="fragment">
							<em>Annoy</em>: a C++/Python library created by Spotify to calculate approximate nearest neighbors.
						</p>
					</section>
				</section>
				<section>
					<section>
						<h3>Main objective</h3>
						<p class="fragment">
							Can we identify the best algorithm for learning resource recomendations?
						</p>
					</section>
					<section>
						<h4>Objective 1</h4>
						<p class="fragment">
							Build a corpus of 30,000 academic papers to act as a proxy for learning resources.
						</p>
					</section>
					<section>
						<h4>Objective 2</h4>
						<p class="fragment">
							Evaluate the temporal performance of each of the algorithms.
						</p>
					</section>
					<section>
						<h4>Objective 3</h4>
						<p class="fragment">
							Rank the quality of recommendations generated by the algorithms.
						</p>
					</section>
					<section>
						<h4>Objective 4</h4>
						<p class="fragment">
							Compare the results to available meta-data.
						</p>
					</section>
				</section>
				<section>
					<h3>Preliminary Results</h3>
				</section>
				<section>
					<section>
						<h3>LDA</h3>
						<p class="fragment">
							A model where documents contain a mixture of topics.
						</p>
						<p class="fragment">
							Topics are a mixture of word probabilities.
						</p>
						<p class="fragment">
							Model assumes that the topics are generated first and then the documents are generated from the topics.
						</p>
						<p class="fragment">
							Inferring the topics is a reversing of the generation process.
						</p>
					</section>
					<section>
						<h3>LDA Results</h3>
						<p>
							TODO: Create graph of results
						</p>
					</section>
				</section>
				<section>
					<section>
						<h3>k-Nearest-Neighbors</h3>
						<p class="fragment">
							A classification algorithm that finds groups of k objects nearest to an input value.
						</p>
						<p class="fragment">
							Assumes that objects that are close in feature space are the same.
						</p>
						<p class="fragment">
							Suffers from the curse of dimensionality when the features space is large.
						</p>
						<p class="fragment">
							Spotify's Annoy approximate-Nearest-Neighbor library was used for k-NN queries.
						</p>
					</section>
					<section>
						<h3>k-NN Results</h3>
						<p>
							TODO: Create graph of results
						</p>
					</section>
				</section>
				<section>
					<section>
						<h3>Word2Vec</h3>
						<p class="fragment">
							An algorithm that uses unsupervised neural-networks to learn the meaning behind words.
						</p>
						<p class="fragment">
							Uses two learning algorithms, continuous bag of words or a continuous skip-gram.
						</p>
						<p class="fragment">
							Can use algebraic operations on the resulting word vectors to find words close in word vector space.
						</p>
					</section>
					<section>
						<h4>Operations</h4>
						<p class="fragment">
							<em>vector('king') - vector('man') + vector('woman') -> 'queen'</em>
						</p>
						<p class="fragment">
							<em>vector('biggest') âˆ’ vector('big') + vector('small') -> 'smallest'</em>
						</p>
						<p class="fragment">
							<em>vector('Berlin') - vector('Germany') + vector('France) - > 'Paris'
							</em>
						</p>
					</section>
					<section>
						<h3>Word2Vec Results</h3>
						<p>
							TODO: Create graph of results
						</p>
					</section>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
